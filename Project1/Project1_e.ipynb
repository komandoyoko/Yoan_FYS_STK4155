{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a85414c2",
   "metadata": {},
   "source": [
    "Part e) Writing our own code for Lasso regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb072b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_gradient(X, y, theta, lam):\n",
    "    grad = (2 / n_samples) * X.T @ (X @ theta - y)\n",
    "    subgrad = lam * np.sign(theta)\n",
    "    subgrad[0] = 0.0  # do not penalize intercept\n",
    "    return grad + subgrad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfcb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_gd(X, y, iterations, momentum, n_steps, func, lam=0.01):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    change = np.zeros_like(theta)\n",
    "    mse_val = np.zeros(iterations)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        if func == \"OLS\":\n",
    "            grad = ols_gradient(X, y, theta)\n",
    "        elif func == \"Ridge\":\n",
    "            grad = ridge_gradient(X, y, theta, lam)\n",
    "        elif func == \"Lasso\":\n",
    "            grad = lasso_gradient(X, y, theta, lam)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown func type\")\n",
    "\n",
    "        change = momentum * change + n_steps * grad\n",
    "        theta -= change\n",
    "        mse_val[i] = mean_squared_error(y, X @ theta)\n",
    "    \n",
    "    return theta, mse_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bcfecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimizer_lasso(method, X, y, lam=0.01):\n",
    "    iterations = 400\n",
    "    n_steps = 0.01\n",
    "    momentum = 0.9\n",
    "    if method == \"GD\":\n",
    "        return momentum_gd(X, y, iterations, 0, n_steps, \"Lasso\", lam)\n",
    "    elif method == \"Momentum\":\n",
    "        return momentum_gd(X, y, iterations, momentum, n_steps, \"Lasso\", lam)\n",
    "    elif method == \"Adagrad\":\n",
    "        return ADAgrad(X, y, iterations, n_steps, \"Lasso\", lam)\n",
    "    elif method == \"RMSProp\":\n",
    "        return RMSProp(X, y, iterations, n_steps, \"Lasso\", lam)\n",
    "    elif method == \"Adam\":\n",
    "        return ADAM(X, y, iterations, n_steps, \"Lasso\", lam)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f9ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_lasso, mse_lasso = run_optimizer_lasso(\"GD\", X_train, y_train, lam=0.01)\n",
    "y_pred_lasso = X_test @ theta_lasso\n",
    "\n",
    "print(\"MSE (Lasso, GD):\", mean_squared_error(y_test, y_pred_lasso))\n",
    "print(\"R2 (Lasso, GD):\", R2_score(y_test, y_pred_lasso))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
