{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c7e6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46bddbab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# MSE gradient\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m MSE_grad = (\u001b[32m2\u001b[39m / \u001b[43mn_samples\u001b[49m) * X.T @ (X @ theta - y)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Ridge gradient (L2 term)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mridge_gradient\u001b[39m(X, y, theta, lam):\n",
      "\u001b[31mNameError\u001b[39m: name 'n_samples' is not defined"
     ]
    }
   ],
   "source": [
    "# MSE gradient\n",
    "MSE_grad = (2 / n_samples) * X.T @ (X @ theta - y)\n",
    "\n",
    "# Ridge gradient (L2 term)\n",
    "def ridge_gradient(X, y, theta, lam):\n",
    "    n_samples = X.shape[0]\n",
    "    return (2/n_samples) * X.T @ (X @ theta - y) + 2 * lam * theta\n",
    "\n",
    "# Lasso gradient (L1 term)\n",
    "def lasso_gradient(X, y, theta, lam):\n",
    "    n_samples = X.shape[0]\n",
    "    grad = (2 / n_samples) * X.T @ (X @ theta - y)\n",
    "    subgrad = lam * np.sign(theta)\n",
    "    subgrad[0] = 0.0\n",
    "    return grad + subgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross-entropy loss gradients \n",
    "import numpy as np\n",
    "\n",
    "def binary_cross_entropy_loss(X, y, beta, lam=0.0, penalty=None):\n",
    "\n",
    "    n = len(y)\n",
    "    z = X @ beta\n",
    "    p = sigmoid(z)\n",
    "    \n",
    "    eps = 1e-15\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    \n",
    "    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "    \n",
    "    if penalty == 'l2':\n",
    "        loss += lam * np.sum(beta**2)\n",
    "    elif penalty == 'l1':\n",
    "        loss += lam * np.sum(np.abs(beta))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def binary_cross_entropy_grad(X, y, beta, lam=0.0, penalty=None):\n",
    "    n = len(y)\n",
    "    p = sigmoid(X @ beta)\n",
    "    grad = (1/n) * X.T @ (p - y)\n",
    "    \n",
    "    if penalty == 'l2':\n",
    "        grad += 2 * lam * beta\n",
    "    elif penalty == 'l1':\n",
    "        grad += lam * np.sign(beta)\n",
    "    \n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10140c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax cross-entropy loss\n",
    "import numpy as np\n",
    "\n",
    "def one_hot(y, K):\n",
    "    n = y.shape[0]\n",
    "    Y = np.zeros((n, K), dtype=float)\n",
    "    Y[np.arange(n), y] = 1.0\n",
    "    return Y\n",
    "\n",
    "def softmax(logits):\n",
    "    z = logits - np.max(logits, axis=1, keepdims=True)\n",
    "    expz = np.exp(z)\n",
    "    return expz / np.sum(expz, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_cross_entropy_loss(\n",
    "    X, Y, W, b=None, lam=0.0, penalty=None, reg_bias=False\n",
    "):\n",
    "    n = X.shape[0]\n",
    "    logits = X @ W + (b if b is not None else 0.0)\n",
    "    P = softmax(logits)\n",
    "\n",
    "    eps = 1e-15\n",
    "    loss = -np.sum(Y * np.log(np.clip(P, eps, 1 - eps))) / n\n",
    "\n",
    "    if penalty == 'l2':\n",
    "        loss += lam * (np.sum(W**2) + (np.sum(b**2) if reg_bias and b is not None else 0.0))\n",
    "    elif penalty == 'l1':\n",
    "        loss += lam * (np.sum(np.abs(W)) + (np.sum(np.abs(b)) if reg_bias and b is not None else 0.0))\n",
    "\n",
    "    return loss, P\n",
    "\n",
    "def softmax_cross_entropy_grads(\n",
    "    X, Y, W, b=None, lam=0.0, penalty=None, reg_bias=False\n",
    "):\n",
    "\n",
    "    n = X.shape[0]\n",
    "    logits = X @ W + (b if b is not None else 0.0)\n",
    "    P = softmax(logits)\n",
    "    diff = (P - Y) / n  \n",
    "\n",
    "    grad_W = X.T @ diff  \n",
    "    grad_b = diff.sum(axis=0) if b is not None else None  \n",
    "\n",
    "    if penalty == 'l2':\n",
    "        grad_W += 2 * lam * W\n",
    "        if reg_bias and b is not None:\n",
    "            grad_b += 2 * lam * b\n",
    "    elif penalty == 'l1':\n",
    "        grad_W += lam * np.sign(W)\n",
    "        if reg_bias and b is not None:\n",
    "            grad_b += lam * np.sign(b)\n",
    "\n",
    "    return grad_W, grad_b\n",
    "\n",
    "def gd_step_softmax(\n",
    "    X, Y, W, b=None, eta=0.1, lam=0.0, penalty=None, reg_bias=False\n",
    "):\n",
    "    gW, gb = softmax_cross_entropy_grads(X, Y, W, b, lam, penalty, reg_bias)\n",
    "    W_new = W - eta * gW\n",
    "    b_new = (b - eta * gb) if b is not None else None\n",
    "    return W_new, b_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16c3331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the sigmoid function and it derivative \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fedf8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the ReLU function and its derivative\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.where(z > 0, z, 0)\n",
    "\n",
    "def ReLU_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b27a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Leaky Relu and its derivative\n",
    "\n",
    "def leaky_ReLU(z, alpha=0.01):\n",
    "    return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "def leaky_ReLU_derivative(z, alpha=0.01):\n",
    "    return np.where(z > 0, 1, alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
