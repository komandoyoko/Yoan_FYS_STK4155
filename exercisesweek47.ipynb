{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc2501d",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html exercisesweek47.do.txt  -->\n",
    "<!-- dom:TITLE: Exercise week 47-48 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae5111",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Exercise week 47-48\n",
    "**November 17-28, 2025**\n",
    "\n",
    "Date: **Deadline is Friday November 28 at midnight**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef837a4",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Overarching aims of the exercises this week\n",
    "\n",
    "The exercise set this week is meant as a summary of many of the\n",
    "central elements in various machine learning algorithms we have discussed throught the semester. You don't need to answer all questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ef66b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Linear and logistic regression methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c9231",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 1:\n",
    "\n",
    "Which of the following is not an assumption of ordinary least squares linear regression?\n",
    "\n",
    "* There is a linearity between predictors/features and target/outout\n",
    "\n",
    "--> * The inputs/features distributed according to a normal/gaussian distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acef906",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 2:\n",
    "\n",
    "The mean squared error cost function for linear regression is convex in the parameters, guaranteeing a unique global minimum. True or False? Motivate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc4a9f5",
   "metadata": {},
   "source": [
    "The cost function is a quadratic function of the parameters θ.Quadratic functions form a bowl-shaped surface in parameter space.\n",
    "- A quadratic function always has a positive semi-definite Hessian matrix\n",
    "- Therefore, it is convex\n",
    "- A convex function has one unique global minimum and no local minima\n",
    "So, gradient descent will not get stuck in a bad local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bf02e",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 3:\n",
    "\n",
    "Which statement about logistic regression is false?\n",
    "\n",
    "* Logistic regression is used for binary classification.\n",
    "\n",
    " * It uses the sigmoid function to map linear scores to probabilities.\n",
    "\n",
    " --> * It has an analytical closed-form solution.\n",
    "\n",
    " * Its log-loss (cross-entropy) is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab306a",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 4:\n",
    "\n",
    "Logistic regression produces a linear decision boundary in the input space. True or False? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcaa0d0",
   "metadata": {},
   "source": [
    "True. Logistic regression produces a linear decision boundary because using the classification threshold p = 0.5 results in a linear equation (theta^T x = 0), which defines a straight separating boundary. The decision boundary can become nonlinear only if we apply nonlinear feature transformations such as polynomial features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d695e6bb",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 5:\n",
    "\n",
    "Give two reasons why logistic regression is preferred over linear regression for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905802a2",
   "metadata": {},
   "source": [
    "1. Logistic regression outputs probabilities between 0 and 1 using the sigmoid function, which makes it suitable for binary classification. Linear regression can output values outside this range, which are not meaningful as probabilities.\n",
    "2. Logistic regression uses a classification threshold and a loss function designed for classification (cross-entropy), leading to better decision boundaries and more stable training compared to linear regression, which minimizes squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c398642",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fac35",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 6:\n",
    "\n",
    "Which statement is not true for fully-connected neural networks?\n",
    "\n",
    "* Without nonlinear activation functions they reduce to a single linear model.\n",
    "\n",
    " * Training relies on backpropagation using the chain rule.\n",
    "\n",
    " * A single hidden layer can approximate any continuous function on a compact set.\n",
    "\n",
    " --> * The loss surface of a deep neural network is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed2727",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 7:\n",
    "\n",
    "Using sigmoid activations in many layers of a deep neural network can cause vanishing gradients. True or False? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1946284",
   "metadata": {},
   "source": [
    "True. Using sigmoid activations in many layers can cause vanishing gradients because the sigmoid function squashes inputs into a narrow range between 0 and 1, where gradients become very small. During backpropagation these small gradients are multiplied repeatedly across layers, shrinking toward zero and preventing weights in earlier layers from updating effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1865d",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 8:\n",
    "\n",
    "Describe the vanishing gradient problem: Why does it occur? Mention one technique to mitigate it and explain briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4985f",
   "metadata": {},
   "source": [
    "The vanishing gradient problem occurs when gradients become extremely small as they are backpropagated through many layers, often due to activation functions like sigmoid or tanh that squash values into a narrow range. This causes early layers to learn very slowly or stop learning altogether. One technique to mitigate it is using ReLU activation functions, which avoid saturation for positive inputs and allow gradients to flow more effectively through the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ad1a8",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 9:\n",
    "\n",
    "Consider a fully-connected network with layer sizes $n_0$ (the input\n",
    "layer) ,$n_1$ (first hidden layer), $\\dots, n_L$, where $n_L$ is the\n",
    "output layer. Derive a general formula for the total number of\n",
    "trainable parameters (weights + biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab95348",
   "metadata": {},
   "source": [
    "Between each pair of layers \\(l-1\\) and \\(l\\), there are \\(n_{l-1} \\cdot n_l\\) weights and \\(n_l\\) biases.  \n",
    "So, the total number of trainable parameters is:\n",
    "\n",
    "$$\n",
    "\\sum_{l=1}^{L} (n_{l-1} \\cdot n_l + n_l)\n",
    "$$\n",
    "\n",
    "This can be written more compactly as:\n",
    "\n",
    "$$\n",
    "\\sum_{l=1}^{L} n_l (n_{l-1} + 1)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2ed47",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d54a83",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 10:\n",
    "\n",
    "Which of the following is not a typical property or advantage of CNNs?\n",
    "\n",
    "* Local receptive fields\n",
    "\n",
    "* Weight sharing\n",
    "\n",
    "-->* More parameters than fully-connected layers\n",
    "\n",
    "* Pooling layers offering some translation invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aefcc46",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 11:\n",
    "\n",
    "Using zero-padding in convolutional layers can preserve the input\n",
    "spatial dimensions when using a $3 \\times 3$ kernel/filter, stride 1,\n",
    "and padding $P = 1$. True or False?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d64c27",
   "metadata": {},
   "source": [
    "True. Using a 3×3 filter with stride 1 and padding P = 1 preserves the spatial dimensions of the input, because padding adds a border that compensates for the reduction that would otherwise occur during convolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b6806",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 12:\n",
    "\n",
    "Given input width $W$, kernel size $K$, stride S, and padding P,\n",
    "derive the formula for the output width $W_{\\text{out}} = \\frac{W - K+ 2P}{S} + 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da571f",
   "metadata": {},
   "source": [
    "Each convolution step moves the filter by `S` pixels. Without padding, the effective input width available for convolution is `W − K + 1`. Adding padding contributes `2P` extra pixels to the usable width.\n",
    "\n",
    "Thus, the number of positions the filter can slide over is:\n",
    "\n",
    "$$\n",
    "W_{\\text{out}} = \\frac{W - K + 2P}{S} + 1\n",
    "$$\n",
    "\n",
    "which gives the formula for the output width of a convolution layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629397f",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 13:\n",
    "\n",
    "A convolutional layer has: $C_{\\text{in}}$ input channels,\n",
    "$C_{\\text{out}}$ output channels (filters) and kernel size $K_h \\times\n",
    "K_w$. Compute the number of trainable parameters including biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd1974",
   "metadata": {},
   "source": [
    "Total parameters =  \n",
    "$$\n",
    "C_{\\text{in}} \\cdot K_h \\cdot K_w \\cdot C_{\\text{out}} + C_{\\text{out}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087780b2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd5f95",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 14:\n",
    "\n",
    "Which statement about simple  RNNs is false?\n",
    "\n",
    "* They maintain a hidden state updated each time step.\n",
    "\n",
    " * They use the same weight matrices at every time step.\n",
    "\n",
    " * They handle sequences of arbitrary length.\n",
    "\n",
    " -->* They eliminate the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70bb6d",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 15:\n",
    "\n",
    "LSTMs mitigate the vanishing gradient problem by using gating mechanisms (input, forget, output gates). True or False? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b78e4b3",
   "metadata": {},
   "source": [
    "True. LSTMs mitigate the vanishing gradient problem by using gating mechanisms (input, forget, and output gates) that control the flow of information and allow gradients to pass through many time steps without shrinking. The cell state enables long-term memory, helping preserve useful gradients during backpropagation through time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ec77a",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 16:\n",
    "\n",
    "What is Backpropagation Through Time (BPTT) and why is it required for training RNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd8849",
   "metadata": {},
   "source": [
    "Backpropagation Through Time (BPTT) is the extension of backpropagation used to train recurrent neural networks. It works by unrolling the RNN across all time steps and then computing gradients through the unfolded network so that errors are propagated backward through time. It is required because the hidden state depends on previous states, so gradients must flow across time steps to update the shared weights correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e01d4",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 17:\n",
    "\n",
    "What does a sliding window do? And why would we use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea2d2f",
   "metadata": {},
   "source": [
    "A sliding window extracts overlapping segments (subsequences) from a longer time series or sequence by moving a fixed-size window step by step across the data. We use it to create multiple training examples from one long sequence, capture local temporal patterns, and enable models like RNNs or CNNs to learn short-term dependencies.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
