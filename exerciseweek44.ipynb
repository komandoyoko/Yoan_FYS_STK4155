{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5534bda7",
   "metadata": {},
   "source": [
    "Exercise 1\n",
    "Leverer som gruppe: Yoan Tutunarov, Sebastian Koranda, Vladislav Foss \n",
    "Sebastian har levert lenke til repository der alle project 2 filene ligger. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744510fe",
   "metadata": {},
   "source": [
    "Exercise 2\n",
    "\n",
    "    a) Liner and logistic regression methods\n",
    "\n",
    "        1. OLS is used to estimate the relationships between dependant and independant variable by drawing a straight line between the data. It tries to fit the best line by minimizing the sum of squared differences between the actual and predicted (by the line) values. Ridge on the other hand, introduces multicollinearity (corellation between independed variable). The regularization term penalizes the coeeficients by pulling them closer to zero. The penalty is controlled by lambda. \n",
    "\n",
    "        2. The dataset used for logistic regression should be binary, meaning that thare are one of two possible outcomes. Continious data is suitable for linear regression. \n",
    "\n",
    "        3. The output in logistic regression is: (y ∈ {0,1}) and this is wrapped in the sigmoid function:         f(x) = 1 / 1 + e^z where z = thata(trnsposed) * x\n",
    "\n",
    "        4. We can not find analytical solution to logistic regression but try to maximize the log-likelihood by iterative optimization such as gradient descent, SGD and so. \n",
    "\n",
    "        5. Since we do not have a lost/cost function we do an optimization by gradient descent. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faaae39",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Exercise 2\n",
    "\n",
    "    b) Deep learning \n",
    "        \n",
    "        1. An activation function is applied to a neuron in NN to decide its output. It takes in the inputs, weight and bias, and adds nonlinearity to produce output. \n",
    "        Sigmoid: 1 / 1 + e^z maps imput to (0, 1) and interprets it as a probality\n",
    "        Relu (Rectified Linear Unit): f(z) = max(0, z) mostly used in hidden layers. Outputs the input directly if its positive and returns zero if negative\n",
    "        Leaky ReLu: solves the dying neuron problem that occurs with Relu, where neurone only outputs zero and stops updating\n",
    "\n",
    "        2. Input layer (input, weight, bias), hidde layers (optional) each with activation function, and output layer with final output function\n",
    "\n",
    "        3. There are several things that can improve/help the model predict. \n",
    "        - Adding regularization as Ridge or Lasso will penalize the large weights, making the model less complex. - Early stoppage can avoid to overfit by reducing its possibility to improve more than enough. \n",
    "        - Adding more that lets the model to generalize rather to remember \n",
    "        - Cross validation: ensures the model generalizes more across multiple subsets \n",
    "\n",
    "        4. If we have many hidden layers it causes the gradients to multiply through the different layers. Then they start to grow a lot. If we get NaN or inf, thats a sing of exploding gradients. \n",
    "\n",
    "        5. Learning rate - decides the size of steps during gradient descent \n",
    "        Number of epochs - too few can cause underfitting, while too many can cause overfitting\n",
    "        Number of layers - adds more complexity \n",
    "        Number of neurons - gives the model ability \n",
    "\n",
    "        6. A Convolutional Neural Network (CNN) is a deep learning architecture that uses convolutional filters to automatically learn and detect spatial features from data like images, reducing the need for manual feature engineering. The architecrure can be: \n",
    "        Input image - Convolution - Activation - Pooling (can have several layers) - Flatten - Fully connected - Output\n",
    "\n",
    "        7. The vanishing gradient problem is when gradients shrink toward zero as they propagate backward, preventing earlier layers from learning.\n",
    "\n",
    "        8. If we have to small lerning rate it may not progress to reduce the cost function. If we have to large learning rate the loss can oscillate. \n",
    "        - Not enough data can cause that the gradient updates may become inconsistent because the training data doesn’t represent the underlying distribution well.\n",
    "        - Unsuitable activation function \n",
    "        - Innapropriate weight initialization \n",
    "\n",
    "        9. Ridge regularization encourages small but non-zero weights. Lasso penalizes many weights to become zero. \n",
    "\n",
    "        10. Deep learning has an advantage over traditional models because it can automatically learn nonlinear, hierarchical representations from raw, complex data — enabling it to solve problems traditional methods cannot (like vision, speech, or language understanding).\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d35e9f",
   "metadata": {},
   "source": [
    "Exercise 2\n",
    "\n",
    "    c) Optimization part\n",
    "\n",
    "        1. The basic mathematical root-finding method behind Gradient descent is to find where the gradient of the loss function reaches 0. \n",
    "\n",
    "        2. Gradient descent is a simplified, first-order approximation to the Newton–Raphson root-finding method.\n",
    "        We introduce the learning rate because we drop the Hessian, so we must manually control the step size to compensate for curvature. Since We replace the \"Hessian matrix\" from the root-finding method with the \"learning rate * Identity matrix\" we loose the curvature information and choose to controll the steps along the gradient manually. \n",
    "\n",
    "        3. Momentum makes updates behave like a low-pass filter on the gradient by smoothing noisy gradients, accumulating gradient directions over time and speeding up the learning along consistent direction.  If momentum is set too close to 1, the optimizer holds on to old gradient information for too long, so it reacts very slowly to new data or changes. It can also build up too much speed and overshoot the minimum, causing it to bounce around instead of settling. In short, training becomes unstable or very slow to learn.\n",
    "\n",
    "        4. The SGD uses batches (small parts of the training data) while the PGD uses the full training data. SGD learn faster, does not require much memory due to not loading the whole dataset and it can espace a local minima bacause each training batch gives different gradient. \n",
    "\n",
    "        5. There are three main parameters that we use and tune.\n",
    "            - learning rate: adjust the step size \n",
    "            - batch size: controls the speed of updates, size of batches affect generaliozation and noise\n",
    "            - momentum (beta): helps speed up the convergence and reduces oscillations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf0fb7c",
   "metadata": {},
   "source": [
    "Exercise 2\n",
    "\n",
    "    c) Analysis of results\n",
    "\n",
    "        1. Overfitting can be caused if the model is too complex (has many parameters) and has few training samples. Also if the training lasts too long without regularization. On the other hand, underfitting is caused by too simple model which is not trained long enough. If the learning rate is too high the, the model could not manage to settle. \n",
    "\n",
    "        2. The training set lets the model learn patterns and fir the data. The validation set is used to tune parameters as leraning rate, regularization and check if the model is overfitting. The final test set computes how well the model performs to an unseen data (in real world).\n",
    "\n",
    "        3. Resampling helps us make the most of limited data by reusing it in smart ways. Common resamling methods are cross-validation, bootstrap and leave-one-out cross-validation. \n",
    "            - CV: split the data in k-folds. Train on k-1 fold and validate on the remaining one. Then repeat for all folds. \n",
    "            - Bootstrap: randomly sample from the dataset to create many new \"bootstrap samples\"\n",
    "            - LOOCV: Similar to k-fold but uses the whole data and thus is suitable for small datasets.  \n",
    "\n",
    "        4. Regularization (Ridge, Lasso) adds a penalty term to the loss function in order to prevent overfitting. It pushes the big weights to smaller values when some of them may be needed to capture real patterns. So regularization makes the model simpler. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
